{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: gams executable '/usr/local/bin/gams' not found.\n"
     ]
    }
   ],
   "source": [
    "import setup_runs as sr\n",
    "import os, os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyDOE as pyd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#export files?\n",
    "export_ed_files_q = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting build of components for experimental design.\n",
      "Check: export_ed_files_q = True\n",
      "Check: n_lhs = 1000\n",
      "Exporting run_id attribute to /Users/jsyme/Documents/Projects/FY20/SWCHE102-1000/git/MultiSector_LTS_Chile/experimental_design/attribute_runs.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting build of components for experimental design.\")\n",
    "\n",
    "\n",
    "#number of lhs samples\n",
    "n_lhs = sr.dict_init[\"n_lhs\"]\n",
    "#set baseline strategy\n",
    "strat_baseline = 0#[x for x in sr.dict_strat.keys() if sr.dict_strat[x][\"strategy_id\"] == 0][0]\n",
    "\n",
    "###   SOME ATTRIBUTE TABLES\n",
    "df_attribute_design_id = pd.read_csv(sr.fp_csv_attribute_design)\n",
    "#reduce\n",
    "df_attribute_design_id = df_attribute_design_id[df_attribute_design_id[\"include\"] == 1]\n",
    "#get time series id\n",
    "df_attribute_time_series_id = pd.read_csv(sr.fp_csv_attribute_time_series)\n",
    "\n",
    "\n",
    "print(\"Check: export_ed_files_q = \" + str(export_ed_files_q))\n",
    "print(\"Check: n_lhs = \" + str(n_lhs))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#################################\n",
    "#    MULTI SECTOR COMPONENTS    #\n",
    "#################################\n",
    "\n",
    "\n",
    "###   GET THE PARAMETER TABLE\n",
    "\n",
    "#read in uncertainty table for additional sectors\n",
    "if not sr.excursion_q:\n",
    "    parameter_table_additional_sectors = pd.read_csv(sr.fp_csv_parameter_ranges)\n",
    "else:\n",
    "    parameter_table_additional_sectors = pd.read_csv(sr.fp_csv_parameter_ranges_for_excursion)\n",
    "#reduce\n",
    "parameter_table_additional_sectors = parameter_table_additional_sectors[parameter_table_additional_sectors[\"type\"].isin([\"incertidumbre\", \"accion\"])]\n",
    "#add field\n",
    "parameter_table_additional_sectors[\"variable_name_lower\"] = [x.lower().replace(\" \", \"_\") for x in list(parameter_table_additional_sectors[\"parameter\"])]\n",
    "#fill nas and set to integer\n",
    "parameter_table_additional_sectors[\"parameter_constant_q\"] = parameter_table_additional_sectors[\"parameter_constant_q\"].fillna(0)\n",
    "parameter_table_additional_sectors[\"parameter_constant_q\"] = np.array(parameter_table_additional_sectors[\"parameter_constant_q\"]).astype(int)\n",
    "#initialize available groups\n",
    "groups_norm = set([int(x) for x in parameter_table_additional_sectors[\"normalize_group\"] if not np.isnan(x)])\n",
    "\n",
    "# IDENTIFY PARAMETERS THAT DO NOT VARY\n",
    "df_apn = parameter_table_additional_sectors[[\"variable_name_lower\", \"min_2050\", \"max_2050\"]].copy().drop_duplicates().reset_index(drop = True)\n",
    "#initialize the set\n",
    "all_params_novary = set({})\n",
    "#loop to build\n",
    "for p in df_apn[\"variable_name_lower\"].unique():\n",
    "    df_apn_tmp = df_apn[df_apn[\"variable_name_lower\"] == p]\n",
    "    #check to see if it doesn't vary\n",
    "    if len(df_apn_tmp) == 1:\n",
    "        if float(df_apn_tmp[\"min_2050\"].iloc[0]) == float(df_apn_tmp[\"max_2050\"].iloc[0]):\n",
    "            if float(df_apn_tmp[\"min_2050\"].iloc[0]) == 1.0:\n",
    "                all_params_novary = all_params_novary | set({p})\n",
    "#all parameters that vary\n",
    "all_params_vary = set(parameter_table_additional_sectors[\"variable_name_lower\"]) - all_params_novary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###   NORMALIZATION GROUP IDS\n",
    "\n",
    "group_id = parameter_table_additional_sectors[[\"type\", \"parameter\", \"normalize_group\"]].drop_duplicates()\n",
    "#build normalize group and lever group ids (for deltas)\n",
    "norm_vec = []\n",
    "\n",
    "if len(groups_norm) > 0:\n",
    "    #starting point for new group\n",
    "    ind_group = max(groups_norm) + 1\n",
    "else:\n",
    "    ind_group = 1\n",
    "    \n",
    "#loop over rows\n",
    "for i in range(0, len(group_id)):\n",
    "    #get current group\n",
    "    group_cur = group_id[\"normalize_group\"].iloc[i]\n",
    "    #test for NaN\n",
    "    if np.isnan(group_cur):\n",
    "        norm_vec = norm_vec + [ind_group]\n",
    "        #next iteration\n",
    "        ind_group += 1\n",
    "    else:\n",
    "        norm_vec = norm_vec + [int(group_cur)]\n",
    "\n",
    "#add to group_id data frame\n",
    "group_id[\"norm_group_id\"] = norm_vec\n",
    "group_id = group_id.reset_index(drop = True)\n",
    "\n",
    "\n",
    "###   LEVER GROUP IDS\n",
    "\n",
    "#temporary data frame to build lever group id with\n",
    "group_id_tmp = group_id[[\"type\", \"norm_group_id\"]].drop_duplicates()\n",
    "group_id_tmp = group_id_tmp[group_id_tmp[\"type\"].isin([\"Accion\", \"accion\"])]\n",
    "group_id_tmp[\"lever_group_id\"] = range(1, len(group_id_tmp) + 1)\n",
    "if \"lever_group_id\" not in group_id.columns:\n",
    "    group_id = pd.merge(group_id, group_id_tmp[[\"norm_group_id\", \"lever_group_id\"]], how = \"left\", left_on = [\"norm_group_id\"], right_on = [\"norm_group_id\"])\n",
    "#reduce\n",
    "group_id = group_id[[\"parameter\", \"type\", \"norm_group_id\", \"lever_group_id\"]]\n",
    "#replace nas\n",
    "group_id = group_id.fillna({\"lever_group_id\": -1})\n",
    "#integer\n",
    "group_id[\"lever_group_id\"] = [int(x) for x in list(group_id[\"lever_group_id\"])]\n",
    "#merge in\n",
    "group_id  = pd.merge(group_id, parameter_table_additional_sectors[[\"parameter\", \"variable_name_lower\"]], how = \"left\", left_on = [\"parameter\"], right_on = [\"parameter\"])\n",
    "group_id = group_id.drop_duplicates()\n",
    "group_id = group_id[[\"parameter\", \"type\", \"variable_name_lower\", \"norm_group_id\", \"lever_group_id\"]]\n",
    "\n",
    "\n",
    "\n",
    "###   MERGE BACK IN TO PTAS AND CLEAN NAMES/DATA\n",
    "\n",
    "#set merge fields and merge\n",
    "fields_merge = list(set(parameter_table_additional_sectors.columns).intersection(set(group_id.columns)))\n",
    "parameter_table_additional_sectors = pd.merge(parameter_table_additional_sectors, group_id, how = \"left\", left_on = fields_merge, right_on = fields_merge)\n",
    "#fields to extract\n",
    "fields_ext = [x for x in parameter_table_additional_sectors if (x not in [\"normalize_group\"])]\n",
    "#clean the data frame\n",
    "parameter_table_additional_sectors = parameter_table_additional_sectors[fields_ext]\n",
    "#fields that are allowed to be all na\n",
    "fields_allow_na = [\"trajgroup_no_vary_q\", \"parameter_constant_q\", \"normalize_group\"]\n",
    "for field in fields_allow_na:\n",
    "    if field in parameter_table_additional_sectors.columns:\n",
    "        parameter_table_additional_sectors[field] = np.array(parameter_table_additional_sectors[field].fillna(0)).astype(int)\n",
    "#drop nas\n",
    "parameter_table_additional_sectors = parameter_table_additional_sectors.dropna(axis = 1, how = \"all\")\n",
    "\n",
    "#dictionary to rename\n",
    "dict_ptas_rename = dict([[x, x.lower().replace(\" \", \"_\")] for x in parameter_table_additional_sectors.columns])\n",
    "#set type\n",
    "parameter_table_additional_sectors = parameter_table_additional_sectors.rename(columns = dict_ptas_rename)\n",
    "\n",
    "#parameters to index\n",
    "fields_add_sec_all_vals = [\"strategy_id\", \"type\", \"parameter\", \"sector\", \"norm_group_id\", \"lever_group_id\"]\n",
    "#get parameter years that are defined\n",
    "param_years_add_sec = [float(x) for x in parameter_table_additional_sectors.columns if x.replace(\".\", \"\").isnumeric()]\n",
    "param_years_add_sec = [int(x) for x in param_years_add_sec if (int(x) == x)]\n",
    "\n",
    "#initialize for all parameters\n",
    "all_vals_add_sec = {\n",
    "    \"param_years\": param_years_add_sec,\n",
    "    \"future_id\": list(range(1, n_lhs + 1)),\n",
    "    \"design_id\": list(df_attribute_design_id[\"design_id\"]),\n",
    "    \"time_series_id\": list(set(df_attribute_time_series_id[\"time_series_id\"]) & set(parameter_table_additional_sectors[\"time_series_id\"]))\n",
    "}\n",
    "#sort some\n",
    "all_vals_add_sec[\"time_series_id\"].sort()\n",
    "\n",
    "#loop\n",
    "for field in fields_add_sec_all_vals:\n",
    "    #set the field name\n",
    "    str_field = field.lower().replace(\" \", \"_\")\n",
    "    #\n",
    "    if field in [\"lever_group_id\", \"norm_group_id\"]:\n",
    "        set_field = set([x for x in parameter_table_additional_sectors[field] if x > 0])\n",
    "    else:\n",
    "        set_field = set(parameter_table_additional_sectors[field])\n",
    "    #update the dictionary\n",
    "    all_vals_add_sec.update({str_field: set_field})\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "###################################\n",
    "#    GENERATE ATTRIBUTE TABLES    #\n",
    "###################################\n",
    "\n",
    "###   RUN ID\n",
    "df_attribute_run_id_0 = [(x, y) for x in sr.df_strat[\"strategy_id\"] for y in ([0] + all_vals_add_sec[\"future_id\"])]\n",
    "df_attribute_run_id = pd.DataFrame(df_attribute_run_id_0)\n",
    "df_attribute_run_id = df_attribute_run_id.rename(columns = {0: \"strategy_id\", 1: \"future_id\"})\n",
    "df_attribute_run_id[\"run_id\"] = range(0, len(df_attribute_run_id))\n",
    "df_attribute_run_id = df_attribute_run_id[[\"run_id\", \"strategy_id\", \"future_id\"]]\n",
    "\n",
    "if export_ed_files_q:\n",
    "    #note/export\n",
    "    print(\"Exporting run_id attribute to \" + sr.fp_csv_attribute_runs)\n",
    "    df_attribute_run_id.to_csv(sr.fp_csv_attribute_runs, index = None)\n",
    "\n",
    "###   MASTER ID\n",
    "df_attribute_master_id = [[x] + [y] + list(z) for x in all_vals_add_sec[\"design_id\"] for y in all_vals_add_sec[\"time_series_id\"] for z in df_attribute_run_id_0]\n",
    "df_attribute_master_id = pd.DataFrame(df_attribute_master_id)\n",
    "df_attribute_master_id = df_attribute_master_id.rename(columns = {0: \"design_id\", 1: \"time_series_id\", 2: \"strategy_id\", 3: \"future_id\"})\n",
    "fields_df_attribute_master_id = list(df_attribute_master_id.columns)\n",
    "df_attribute_master_id[\"master_id\"] = list(range(0, len(df_attribute_master_id)))\n",
    "df_attribute_master_id = df_attribute_master_id[[\"master_id\"] + fields_df_attribute_master_id]\n",
    "df_attribute_master_id = pd.merge(df_attribute_master_id, df_attribute_run_id, how = \"left\", on = [\"strategy_id\", \"future_id\"])\n",
    "#fields to order by\n",
    "fields_ord_dfm = [\"master_id\", \"design_id\", \"time_series_id\", \"run_id\", \"strategy_id\", \"future_id\"]\n",
    "#reorder\n",
    "df_attribute_master_id = df_attribute_master_id[fields_ord_dfm].sort_values(by = fields_ord_dfm)\n",
    "\n",
    "if export_ed_files_q and not sr.tornado_q:\n",
    "    #note/export\n",
    "    print(\"Exporting master_id attribute to \" + sr.fp_csv_attribute_master)\n",
    "    #export\n",
    "    df_attribute_master_id.to_csv(sr.fp_csv_attribute_master, index = None, encoding = \"UTF-8\")\n",
    "    #export for gams\n",
    "    df_attribute_master_id_gams = df_attribute_master_id[[\"master_id\"]].copy().rename(columns = {\"master_id\": \"Escenarios\"})\n",
    "    df_attribute_master_id_gams.to_csv(sr.fp_csv_gams_data_set_escenarios, index = None, encoding = \"UTF-8\")\n",
    "    del df_attribute_master_id_gams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tornado design...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building tornado design...\\n\")\n",
    "\n",
    "####################################################\n",
    "###                                              ###\n",
    "###    BUILD TORNADO PLOT EXPERIMENTAL DESIGN    ###\n",
    "###                                              ###\n",
    "####################################################\n",
    "\n",
    "#id fields\n",
    "fields_id_ptas_internal = [\"norm_group_id\", \"lever_group_id\"]\n",
    "fields_id_ptas = [x for x in parameter_table_additional_sectors.columns if (x[-3:] == \"_id\") and (x not in fields_id_ptas_internal)]\n",
    "tuples_id_ptas = parameter_table_additional_sectors[fields_id_ptas].drop_duplicates()\n",
    "tuples_id_ptas = [tuple(x) for x in np.array(tuples_id_ptas)]\n",
    "\n",
    "field_param = \"variable_name_lower\"\n",
    "fields_param_years_add_sec = [str(x) for x in param_years_add_sec]\n",
    "\n",
    "#mix vector\n",
    "y_0 = int(sr.dict_init[\"add_sec_variation_start_year\"]) - 1\n",
    "y_1 = max(param_years_add_sec)\n",
    "y_base = min(param_years_add_sec)\n",
    "vec_ramp_unc = sr.build_linear_mix_vec((y_0, y_1), (y_base, y_1))\n",
    "\n",
    "#initialize loop for master id\n",
    "m_id = 0\n",
    "f_id = 0\n",
    "#initialize attribute master id out\n",
    "df_master_out = []\n",
    "\n",
    "init_ed_q = True\n",
    "init_fut_q = True\n",
    "\n",
    "df_out = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = tuples_id_ptas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the dataframe\n",
    "params_tmp = parameter_table_additional_sectors.copy()\n",
    "#reduce the dataframe t the approporiate subset\n",
    "for j in range(len(fields_id_ptas)):\n",
    "    #get the field and reduce the dataframe\n",
    "    field = fields_id_ptas[j]\n",
    "    params_tmp = params_tmp[params_tmp[field] == ti[j]]\n",
    "params_tmp = params_tmp.sort_values(by = [field_param])\n",
    "\n",
    "#data frame to use in copying\n",
    "df_pt = params_tmp.copy().reset_index(drop = True)\n",
    "fields_new = list(df_pt[field_param])\n",
    "df_pt = df_pt[fields_param_years_add_sec].transpose()\n",
    "\n",
    "dict_conv = dict([x for x in zip(list(df_pt.columns), fields_new)])\n",
    "df_pt = df_pt.rename(columns = dict_conv).reset_index(drop = True)\n",
    "df_pt[\"year\"] = [int(x) for x in fields_param_years_add_sec]\n",
    "\n",
    "#finally, commit changes to params_tmp\n",
    "params_tmp = params_tmp[params_tmp[field_param].isin(all_params_vary)]\n",
    "apv = list(params_tmp[field_param])\n",
    "\n",
    "##  split out max and min\n",
    "df_pt_max = params_tmp[[\"sector\", field_param, \"parameter_constant_q\", \"max_2050\"] + fields_param_years_add_sec].copy().rename(columns = {\"max_2050\": \"range\"})\n",
    "df_pt_min = params_tmp[[\"sector\", field_param, \"parameter_constant_q\", \"min_2050\"] + fields_param_years_add_sec].copy().rename(columns = {\"min_2050\": \"range\"})\n",
    "\n",
    "#add fields\n",
    "df_pt_max[\"type\"] = [\"max\" for x in range(len(df_pt_max))]\n",
    "df_pt_min[\"type\"] = [\"min\" for x in range(len(df_pt_min))]\n",
    "\n",
    "#build parameters df and initialize futures\n",
    "params_tmp = pd.concat([df_pt_max, df_pt_min], axis = 0).sort_values(by = [field_param, \"type\"]).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if init_fut_q:\n",
    "    df_future_out = params_tmp[[\"sector\", field_param, \"type\", \"range\", \"parameter_constant_q\"]].copy().reset_index(drop = True)\n",
    "    df_future_out = df_future_out\n",
    "    df_future_out[\"future_id\"] = list(range(1, len(df_future_out) + 1))\n",
    "    #build dict\n",
    "    dict_fut = dict([[tuple(x[0:2]), int(x[2])] for x in np.array(df_future_out[[field_param, \"type\", \"future_id\"]])])\n",
    "    #reorder\n",
    "    df_future_out = df_future_out[[\"future_id\", \"sector\", field_param, \"type\", \"range\", \"parameter_constant_q\"]].rename(columns = {field_param: \"parameter\", \"range\": \"scale_value\", \"type\": \"range_value\"})\n",
    "    init_fut_q = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trajgroup_1-lhs',\n",
       " 'trajgroup_1-lhs',\n",
       " 'trajgroup_2-lhs',\n",
       " 'trajgroup_2-lhs',\n",
       " 'trajgroup_3-lhs',\n",
       " 'trajgroup_3-lhs',\n",
       " 'trajgroup_4-lhs',\n",
       " 'trajgroup_4-lhs',\n",
       " 'trajgroup_5-lhs',\n",
       " 'trajgroup_5-lhs',\n",
       " 'trajgroup_6-lhs',\n",
       " 'trajgroup_6-lhs',\n",
       " 'trajgroup_7-lhs',\n",
       " 'trajgroup_7-lhs',\n",
       " 'trajgroup_8-lhs',\n",
       " 'trajgroup_8-lhs',\n",
       " 'trajgroup_9-lhs',\n",
       " 'trajgroup_9-lhs']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in df_future_out[\"parameter\"] if \"trajgroup\" in x]\n",
    "#[x for x in params_tmp[\"variable_name_lower\"] if \"trajgroup\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_future_out[[\"trajgroup\" in x for x in list(df_future_out[\"parameter\"])]]\n",
    "#array of baseline trajectories\n",
    "array_traj = np.array(params_tmp[fields_param_years_add_sec])\n",
    "#rows of this will be the ramp if the parameter is not constant, and 1 otherwise; apply to uncertinaty delta\n",
    "array_mix = np.array([int(x)*vec_ramp_unc + (1 - int(x))*np.ones(len(fields_param_years_add_sec)) for x in (params_tmp[\"parameter_constant_q\"] == 0)])\n",
    "#ranges and scalar vector\n",
    "vec_ranges = np.array(params_tmp[\"range\"])\n",
    "vec_scale = vec_ranges*np.array(params_tmp[\"2050\"])\n",
    "vec_unc_delta = vec_scale - np.array(params_tmp[\"2050\"])\n",
    "#baseline trajectory + the\n",
    "array_new = array_traj + (array_mix.transpose() * vec_unc_delta).transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([200, 201]),)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(df_future_out[\"parameter\"]) == \"trajgroup_1-lhs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>future_id</th>\n",
       "      <th>sector</th>\n",
       "      <th>parameter</th>\n",
       "      <th>range_value</th>\n",
       "      <th>scale_value</th>\n",
       "      <th>parameter_constant_q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>213</td>\n",
       "      <td>transport</td>\n",
       "      <td>trajgroup_7-lhs</td>\n",
       "      <td>max</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>214</td>\n",
       "      <td>transport</td>\n",
       "      <td>trajgroup_7-lhs</td>\n",
       "      <td>min</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     future_id     sector        parameter range_value  scale_value  \\\n",
       "212        213  transport  trajgroup_7-lhs         max          1.0   \n",
       "213        214  transport  trajgroup_7-lhs         min          0.0   \n",
       "\n",
       "     parameter_constant_q  \n",
       "212                     1  \n",
       "213                     1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_future_out[df_future_out[\"parameter\"] == \"trajgroup_7-lhs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = parameter_table_additional_sectors[parameter_table_additional_sectors[\"strategy_id\"].isin([0])]\n",
    "pt = pt[pt[\"time_series_id\"].isin([0])]\n",
    "tg_params = set([sr.clean_trajgroup_names(x)[\"param\"] for x in list(pt[\"parameter\"]) if (\"trajgroup\" in x) and (\"lhs\" not in x)])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
