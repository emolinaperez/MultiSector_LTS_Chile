{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting build of components for experimental design.\n",
      "Check: export_ed_files_q = True\n",
      "Check: n_lhs = 100\n",
      "\n",
      "\n",
      "param_years_add_sec:\n",
      "[2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050]\n",
      "Index(['sector', 'time_series_id', 'strategy_id', 'type', 'parameter',\n",
      "       'parameter_constant_q', 'min_2050', 'max_2050', '2015', '2016', '2017',\n",
      "       '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026',\n",
      "       '2027', '2028', '2029', '2030', '2031', '2032', '2033', '2034', '2035',\n",
      "       '2036', '2037', '2038', '2039', '2040', '2041', '2042', '2043', '2044',\n",
      "       '2045', '2046', '2047', '2048', '2049', '2050', 'variable_name_lower',\n",
      "       'norm_group_id', 'lever_group_id'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "Exporting run_id attribute to /Users/jsyme/Documents/Projects/FY20/SWCHE102-1000/git/MultiSector_LTS_Chile/experimental_design/attribute_runs.csv\n",
      "Exporting master_id attribute to /Users/jsyme/Documents/Projects/FY20/SWCHE102-1000/git/MultiSector_LTS_Chile/experimental_design/attribute_master.csv\n",
      "Generating LHS Matrix...\n",
      "\n",
      "\n",
      "#####\n",
      "#####    READING IN LHS TABLES\n",
      "#####\n",
      "\n",
      "add_sec LHS table successfully read from /Users/jsyme/Documents/Projects/FY20/SWCHE102-1000/git/MultiSector_LTS_Chile/experimental_design/lhs_samples_multi_sector.csv\n",
      "levers LHS table successfully read from /Users/jsyme/Documents/Projects/FY20/SWCHE102-1000/git/MultiSector_LTS_Chile/experimental_design/lhs_samples_levers.csv\n",
      "\n",
      "LHS complete.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################\n",
      "fields_ordered_parameters:\n",
      "\tcement_emission_fact_coal\n",
      "\tcement_emission_fact_diesel\n",
      "\tcement_emission_fact_kerosene\n",
      "\tcement_emission_fact_natural_gas\n",
      "\tcement_frac_biomass\n",
      "\tcement_frac_coal\n",
      "\tcement_frac_diesel\n",
      "\tcement_frac_electric\n",
      "\tcement_frac_hydrogen\n",
      "\tcement_frac_kerosene\n",
      "\tcement_frac_natural_gas\n",
      "\tcement_frac_solar\n",
      "\tcement_frac_thermal_solar\n",
      "\tcement_intensity\n",
      "\tcement_production\n",
      "\tcommercial_elasticity\n",
      "\tcommercial_emission_fact_diesel\n",
      "\tcommercial_emission_fact_kerosene\n",
      "\tcommercial_emission_fact_natural_gas\n",
      "\tcommercial_emission_fact_pliqgas\n",
      "\tcommercial_frac_biomass\n",
      "\tcommercial_frac_diesel\n",
      "\tcommercial_frac_electric\n",
      "\tcommercial_frac_hydrogen\n",
      "\tcommercial_frac_kerosene\n",
      "\tcommercial_frac_natural_gas\n",
      "\tcommercial_frac_pliqgas\n",
      "\tcommercial_frac_solar\n",
      "\tcopper_efficiency_heat_diesel\n",
      "\tcopper_efficiency_heat_electricitiy\n",
      "\tcopper_efficiency_heat_hydrogen\n",
      "\tcopper_efficiency_heat_natural_gas\n",
      "\tcopper_efficiency_heat_plqgas\n",
      "\tcopper_efficiency_heat_solar\n",
      "\tcopper_efficiency_motor_diesel\n",
      "\tcopper_efficiency_motor_electricitiy\n",
      "\tcopper_efficiency_motor_hydrogen\n",
      "\tcopper_efficiency_open_pit_mine_diesel\n",
      "\tcopper_efficiency_open_pit_mine_electricitiy\n",
      "\tcopper_efficiency_open_pit_mine_hydrogen\n",
      "\tcopper_efficiency_other_diesel\n",
      "\tcopper_efficiency_other_electricity\n",
      "\tcopper_efficiency_other_natural_gas\n",
      "\tcopper_efficiency_subt_mine_diesel\n",
      "\tcopper_efficiency_subt_mine_electricitiy\n",
      "\tcopper_efficiency_subt_mine_hydrogen\n",
      "\tcopper_emission_fact_diesel\n",
      "\tcopper_emission_fact_fueloil\n",
      "\tcopper_emission_fact_kerosene\n",
      "\tcopper_emission_fact_natural_gas\n",
      "\tcopper_emission_fact_pliqgas\n",
      "\tcopper_heat_diesel\n",
      "\tcopper_heat_electricitiy\n",
      "\tcopper_heat_hydrogen\n",
      "\tcopper_heat_natural_gas\n",
      "\tcopper_heat_plqgas\n",
      "\tcopper_heat_solar\n",
      "\tcopper_intensity_useful_energy\n",
      "\tcopper_motor_diesel\n",
      "\tcopper_motor_electricitiy\n",
      "\tcopper_motor_hydrogen\n",
      "\tcopper_open_pit_mine_diesel\n",
      "\tcopper_open_pit_mine_electricitiy\n",
      "\tcopper_open_pit_mine_hydrogen\n",
      "\tcopper_other_diesel\n",
      "\tcopper_other_electricity\n",
      "\tcopper_other_natural_gas\n",
      "\tcopper_share_heat\n",
      "\tcopper_share_motor\n",
      "\tcopper_share_open_pit_mine\n",
      "\tcopper_share_other\n",
      "\tcopper_share_subt_mine\n",
      "\tcopper_subt_mine_diesel\n",
      "\tcopper_subt_mine_electricitiy\n",
      "\tcopper_subt_mine_hydrogen\n",
      "\tcumplimiento_metas\n",
      "\tef_captura\n",
      "\telast_res_ind_pib\n",
      "\telectrolyzer_efficiency\n",
      "\tfishing_emission_fact_coal\n",
      "\tfishing_emission_fact_diesel\n",
      "\tfishing_emission_fact_natural_gas\n",
      "\tfishing_frac_biomass\n",
      "\tfishing_frac_coal\n",
      "\tfishing_frac_diesel\n",
      "\tfishing_frac_electric\n",
      "\tfishing_frac_hydrogen\n",
      "\tfishing_frac_natural_gas\n",
      "\tfishing_frac_solar\n",
      "\tfishing_frac_thermal_solar\n",
      "\tfishing_intensity\n",
      "\tfishing_production\n",
      "\tgdp\n",
      "\tgrowth_rate_gdp\n",
      "\thfcmma\n",
      "\thydrology_exceedance_probability\n",
      "\tincendio_bosque_nativo\n",
      "\tincendio_plantaciones_forestales\n",
      "\tincertidumbre_incendio_bn\n",
      "\tincertidumbre_incendio_plantaciones\n",
      "\tiron_emission_fact_coal\n",
      "\tiron_emission_fact_diesel\n",
      "\tiron_emission_fact_natural_gas\n",
      "\tiron_frac_biomass\n",
      "\tiron_frac_coal\n",
      "\tiron_frac_diesel\n",
      "\tiron_frac_electric\n",
      "\tiron_frac_hydrogen\n",
      "\tiron_frac_natural_gas\n",
      "\tiron_frac_solar\n",
      "\tiron_frac_thermal_solar\n",
      "\tiron_intensity\n",
      "\tmedida_biodigestores\n",
      "\tmedida_cambio_dieta_bovina\n",
      "\tmedida_captura_c_suelos\n",
      "\tmedida_uso_eficiente_fertilizante\n",
      "\tnueva_cap_recuperacion\n",
      "\tother_industries_efficiency_heat_biomass\n",
      "\tother_industries_efficiency_heat_coal\n",
      "\tother_industries_efficiency_heat_diesel\n",
      "\tother_industries_efficiency_heat_electric\n",
      "\tother_industries_efficiency_heat_fuel_oil\n",
      "\tother_industries_efficiency_heat_hydrogen\n",
      "\tother_industries_efficiency_heat_natural_gas\n",
      "\tother_industries_efficiency_heat_pliqgas\n",
      "\tother_industries_efficiency_heat_solar\n",
      "\tother_industries_efficiency_motor_diesel\n",
      "\tother_industries_efficiency_motor_electric\n",
      "\tother_industries_efficiency_motor_hydrogen\n",
      "\tother_industries_efficiency_motor_pliqgas\n",
      "\tother_industries_efficiency_other_electric\n",
      "\tother_industries_elasticity\n",
      "\tother_industries_emission_fact_coal\n",
      "\tother_industries_emission_fact_diesel\n",
      "\tother_industries_emission_fact_fueloil\n",
      "\tother_industries_emission_fact_natural_gas\n",
      "\tother_industries_emission_fact_pliqgas\n",
      "\tother_industries_frac_biomass\n",
      "\tother_industries_frac_coal\n",
      "\tother_industries_frac_diesel\n",
      "\tother_industries_frac_electric\n",
      "\tother_industries_frac_hydrogen\n",
      "\tother_industries_frac_natural_gas\n",
      "\tother_industries_frac_solar\n",
      "\tother_industries_frac_thermal_solar\n",
      "\tother_industries_heat_biomass\n",
      "\tother_industries_heat_coal\n",
      "\tother_industries_heat_diesel\n",
      "\tother_industries_heat_electric\n",
      "\tother_industries_heat_fuel_oil\n",
      "\tother_industries_heat_hydrogen\n",
      "\tother_industries_heat_natural_gas\n",
      "\tother_industries_heat_pliqgas\n",
      "\tother_industries_heat_solar\n",
      "\tother_industries_intensity\n",
      "\tother_industries_motor_diesel\n",
      "\tother_industries_motor_electric\n",
      "\tother_industries_motor_hydrogen\n",
      "\tother_industries_motor_pliqgas\n",
      "\tother_industries_other_electric\n",
      "\tother_industries_rate_useful_energy\n",
      "\tother_industries_share_heat\n",
      "\tother_industries_share_motor\n",
      "\tother_industries_share_other\n",
      "\tother_mining_emission_fact_coal\n",
      "\tother_mining_emission_fact_diesel\n",
      "\tother_mining_emission_fact_natural_gas\n",
      "\tother_mining_frac_biomass\n",
      "\tother_mining_frac_coal\n",
      "\tother_mining_frac_diesel\n",
      "\tother_mining_frac_electric\n",
      "\tother_mining_frac_hydrogen\n",
      "\tother_mining_frac_natural_gas\n",
      "\tother_mining_frac_solar\n",
      "\tother_mining_frac_thermal_solar\n",
      "\tother_mining_intensity\n",
      "\tother_mining_production\n",
      "\tpib\n",
      "\tpoblacion\n",
      "\tpoblacion_rural\n",
      "\tpoblacion_urbana\n",
      "\tpqe_vehicular\n",
      "\tprecio_bovino\n",
      "\tprecio_cerdo\n",
      "\tprecio_leche\n",
      "\tprecio_maiz\n",
      "\tprecio_pollo\n",
      "\tprod_cemento\n",
      "\tptas_antofa\n",
      "\tptas_biobio\n",
      "\tptas_valpo\n",
      "\tptasls_coq\n",
      "\tpublic_emission_fact_kerosene\n",
      "\tpublic_emission_fact_natural_gas\n",
      "\tpublic_emission_fact_pliqgas\n",
      "\tpublic_frac_biomass\n",
      "\tpublic_frac_diesel\n",
      "\tpublic_frac_electric\n",
      "\tpublic_frac_kerosene\n",
      "\tpublic_frac_natural_gas\n",
      "\tpublic_frac_pliqgas\n",
      "\tpublic_frac_solar\n",
      "\tpublic_intensity\n",
      "\tpulp_emission_fact_diesel\n",
      "\tpulp_emission_fact_natural_gas\n",
      "\tpulp_frac_biomass\n",
      "\tpulp_frac_diesel\n",
      "\tpulp_frac_electric\n",
      "\tpulp_frac_hydrogen\n",
      "\tpulp_frac_natural_gas\n",
      "\tpulp_frac_solar\n",
      "\tpulp_frac_thermal_solar\n",
      "\tpulp_intensity\n",
      "\trecuperacion_bn\n",
      "\tresidential_emission_fact_kerosene\n",
      "\tresidential_emission_fact_natural_gas\n",
      "\tresidential_emission_fact_pliqgas\n",
      "\tresidential_frac_biomass\n",
      "\tresidential_frac_electric\n",
      "\tresidential_frac_hydrogen\n",
      "\tresidential_frac_kerosene\n",
      "\tresidential_frac_natural_gas\n",
      "\tresidential_frac_pliqgas\n",
      "\tresidential_frac_solar\n",
      "\tresidential_intensity\n",
      "\tresidential_occ_rate\n",
      "\trs_arauc\n",
      "\trs_aysen\n",
      "\trs_coq\n",
      "\trs_magallanes\n",
      "\trs_tarapaca\n",
      "\trs_valpo\n",
      "\tsaltpeter_emission_fact_coal\n",
      "\tsaltpeter_emission_fact_diesel\n",
      "\tsaltpeter_emission_fact_natural_gas\n",
      "\tsaltpeter_frac_biomass\n",
      "\tsaltpeter_frac_coal\n",
      "\tsaltpeter_frac_diesel\n",
      "\tsaltpeter_frac_electric\n",
      "\tsaltpeter_frac_hydrogen\n",
      "\tsaltpeter_frac_natural_gas\n",
      "\tsaltpeter_frac_solar\n",
      "\tsaltpeter_frac_thermal_solar\n",
      "\tsaltpeter_intensity\n",
      "\tsaltpeter_production\n",
      "\tshare_electric_grid_to_hydrogen\n",
      "\tsteel_emission_fact_coal\n",
      "\tsteel_emission_fact_diesel\n",
      "\tsteel_emission_fact_kerosene\n",
      "\tsteel_emission_fact_natural_gas\n",
      "\tsteel_frac_biomass\n",
      "\tsteel_frac_coal\n",
      "\tsteel_frac_diesel\n",
      "\tsteel_frac_electric\n",
      "\tsteel_frac_hydrogen\n",
      "\tsteel_frac_kerosene\n",
      "\tsteel_frac_natural_gas\n",
      "\tsteel_frac_solar\n",
      "\tsteel_frac_thermal_solar\n",
      "\tsteel_intensity\n",
      "\tsteel_production\n",
      "\tsugar_emission_fact_coal\n",
      "\tsugar_emission_fact_diesel\n",
      "\tsugar_emission_fact_natural_gas\n",
      "\tsugar_frac_biomass\n",
      "\tsugar_frac_coal\n",
      "\tsugar_frac_diesel\n",
      "\tsugar_frac_electric\n",
      "\tsugar_frac_hydrogen\n",
      "\tsugar_frac_natural_gas\n",
      "\tsugar_frac_solar\n",
      "\tsugar_frac_thermal_solar\n",
      "\tsugar_intensity\n",
      "\tsugar_production\n",
      "\tsupuestos_mma_ippu\n",
      "\tsupuestos_mma_waste\n",
      "\tsustitucion_degradacion\n",
      "\ttasa_fuga\n",
      "\ttierras_convertidas_a_bn\n",
      "\ttierras_convertidas_a_plantaciones\n",
      "\ttrajmax_copper_production\n",
      "\ttrajmax_fuel_price_coal\n",
      "\ttrajmax_fuel_price_diesel\n",
      "\ttrajmax_fuel_price_fuel_oil\n",
      "\ttrajmax_fuel_price_natural_gas\n",
      "\ttrajmax_investment_cost_csp_solar\n",
      "\ttrajmax_investment_cost_geothermal\n",
      "\ttrajmax_investment_cost_natural_gas_cc\n",
      "\ttrajmax_investment_cost_pv_solar\n",
      "\ttrajmax_investment_cost_wind\n",
      "\ttrajmax_iron_production\n",
      "\ttrajmax_pulp_production\n",
      "\ttrajmin_copper_production\n",
      "\ttrajmin_fuel_price_coal\n",
      "\ttrajmin_fuel_price_diesel\n",
      "\ttrajmin_fuel_price_fuel_oil\n",
      "\ttrajmin_fuel_price_natural_gas\n",
      "\ttrajmin_investment_cost_csp_solar\n",
      "\ttrajmin_investment_cost_geothermal\n",
      "\ttrajmin_investment_cost_natural_gas_cc\n",
      "\ttrajmin_investment_cost_pv_solar\n",
      "\ttrajmin_investment_cost_wind\n",
      "\ttrajmin_iron_production\n",
      "\ttrajmin_pulp_production\n",
      "\ttrajmix_copper_production\n",
      "\ttrajmix_fuel_price_coal\n",
      "\ttrajmix_fuel_price_diesel\n",
      "\ttrajmix_fuel_price_fuel_oil\n",
      "\ttrajmix_fuel_price_natural_gas\n",
      "\ttrajmix_investment_cost_csp_solar\n",
      "\ttrajmix_investment_cost_geothermal\n",
      "\ttrajmix_investment_cost_natural_gas_cc\n",
      "\ttrajmix_investment_cost_pv_solar\n",
      "\ttrajmix_investment_cost_wind\n",
      "\ttrajmix_iron_production\n",
      "\ttrajmix_pulp_production\n",
      "\ttransport_bus_veh_km\n",
      "\ttransport_emission_fact_diesel\n",
      "\ttransport_emission_fact_fueloil\n",
      "\ttransport_emission_fact_gasoline\n",
      "\ttransport_emission_fact_kerosene_aviation\n",
      "\ttransport_frac_bus_diesel\n",
      "\ttransport_frac_bus_electric\n",
      "\ttransport_frac_private_diesel\n",
      "\ttransport_frac_private_electric\n",
      "\ttransport_frac_private_gasoline\n",
      "\ttransport_frac_taxi_electric\n",
      "\ttransport_frac_taxi_gasoline\n",
      "\ttransport_frac_truck_diesel\n",
      "\ttransport_frac_truck_hydrogen\n",
      "\ttransport_intensity_aviation_kerosene\n",
      "\ttransport_intensity_bus_diesel\n",
      "\ttransport_intensity_bus_electric\n",
      "\ttransport_intensity_maritime\n",
      "\ttransport_intensity_pkm_train\n",
      "\ttransport_intensity_private_diesel\n",
      "\ttransport_intensity_private_electric\n",
      "\ttransport_intensity_private_gasoline\n",
      "\ttransport_intensity_taxi_electric\n",
      "\ttransport_intensity_taxi_gasoline\n",
      "\ttransport_intensity_train_electric\n",
      "\ttransport_intensity_train_freight_diesel\n",
      "\ttransport_intensity_truck_diesel\n",
      "\ttransport_intensity_truck_hydrogen\n",
      "\ttransport_load_average_freight_train\n",
      "\ttransport_load_average_truck\n",
      "\ttransport_maritime_frac_diesel\n",
      "\ttransport_maritime_frac_fueloil\n",
      "\ttransport_modal_split_private_to_bus\n",
      "\ttransport_modal_split_private_to_cycling\n",
      "\ttransport_modal_split_private_to_taxi\n",
      "\ttransport_modal_split_private_to_telework\n",
      "\ttransport_modal_split_private_to_train\n",
      "\ttransport_modal_split_truck\n",
      "\ttransport_modal_split_truck_to_train\n",
      "\ttransport_occupancy_rate_bus\n",
      "\ttransport_occupancy_rate_private\n",
      "\ttransport_occupancy_rate_taxi\n",
      "\ttransport_occupancy_rate_train\n",
      "\ttransport_pkm_aviation\n",
      "\ttransport_saturation_aviation\n",
      "\ttransport_share_diesel_private\n",
      "\ttransport_share_diesel_truck\n",
      "\ttransport_tkm_freight_train\n",
      "\ttransport_tkm_maritime\n",
      "\ttransport_train13_pkm\n",
      "\ttransport_train5_pkm\n",
      "\ttransport_train8_pkm\n",
      "\ttransport_trip_distance_private\n",
      "\tvar_demanda_legna\n",
      "##############################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################\n",
      "all_constant_params:\n",
      "\tcumplimiento_metas\n",
      "\telast_res_ind_pib\n",
      "\thfcmma\n",
      "\thydrology_exceedance_probability\n",
      "\tincendio_bosque_nativo\n",
      "\tincendio_plantaciones_forestales\n",
      "\tmedida_biodigestores\n",
      "\tmedida_cambio_dieta_bovina\n",
      "\tmedida_captura_c_suelos\n",
      "\tmedida_uso_eficiente_fertilizante\n",
      "\tnueva_cap_recuperacion\n",
      "\tptas_antofa\n",
      "\tptas_biobio\n",
      "\tptas_valpo\n",
      "\tptasls_coq\n",
      "\trecuperacion_bn\n",
      "\trs_arauc\n",
      "\trs_aysen\n",
      "\trs_coq\n",
      "\trs_magallanes\n",
      "\trs_tarapaca\n",
      "\trs_valpo\n",
      "\tsupuestos_mma_ippu\n",
      "\tsupuestos_mma_waste\n",
      "\tsustitucion_degradacion\n",
      "\ttasa_fuga\n",
      "\ttierras_convertidas_a_bn\n",
      "\ttierras_convertidas_a_plantaciones\n",
      "\ttrajmix_copper_production\n",
      "\ttrajmix_fuel_price_coal\n",
      "\ttrajmix_fuel_price_diesel\n",
      "\ttrajmix_fuel_price_fuel_oil\n",
      "\ttrajmix_fuel_price_natural_gas\n",
      "\ttrajmix_investment_cost_csp_solar\n",
      "\ttrajmix_investment_cost_geothermal\n",
      "\ttrajmix_investment_cost_natural_gas_cc\n",
      "\ttrajmix_investment_cost_pv_solar\n",
      "\ttrajmix_investment_cost_wind\n",
      "\ttrajmix_iron_production\n",
      "\ttrajmix_pulp_production\n",
      "##############################\n",
      "\n",
      "\n",
      "\n",
      "Starting generation of lever deltas.\n",
      "\n",
      "Starting build of deltas by time series/strategy...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping of LHS table complete for design_id: 0, time_series_id: 0, strategy_id: 1\n",
      "Reshaping of LHS table complete for design_id: 1, time_series_id: 0, strategy_id: 1\n",
      "Reshaping of LHS table complete for design_id: 0, time_series_id: 1, strategy_id: 1\n",
      "Reshaping of LHS table complete for design_id: 1, time_series_id: 1, strategy_id: 1\n"
     ]
    }
   ],
   "source": [
    "import setup_runs as sr\n",
    "import os, os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyDOE as pyd\n",
    "\n",
    "#export files?\n",
    "export_ed_files_q = True\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "###                                    ###\n",
    "###    GENERATE EXPERIMENTAL DESIGN    ###\n",
    "###                                    ###\n",
    "##########################################\n",
    "\n",
    "print(\"Starting build of components for experimental design.\")\n",
    "\n",
    "\n",
    "#number of lhs samples\n",
    "n_lhs = sr.dict_init[\"n_lhs\"]\n",
    "#set baseline strategy\n",
    "strat_baseline = 0#[x for x in sr.dict_strat.keys() if sr.dict_strat[x][\"strategy_id\"] == 0][0]\n",
    "\n",
    "###   SOME ATTRIBUTE TABLES\n",
    "df_attribute_design_id = pd.read_csv(sr.fp_csv_attribute_design)\n",
    "#reduce\n",
    "df_attribute_design_id = df_attribute_design_id[df_attribute_design_id[\"include\"] == 1]\n",
    "#get time series id\n",
    "df_attribute_time_series_id = pd.read_csv(sr.fp_csv_attribute_time_series)\n",
    "\n",
    "\n",
    "print(\"Check: export_ed_files_q = \" + str(export_ed_files_q))\n",
    "print(\"Check: n_lhs = \" + str(n_lhs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "#    MULTI SECTOR COMPONENTS    #\n",
    "#################################\n",
    "\n",
    "\n",
    "###   GET THE PARAMETER TABLE\n",
    "\n",
    "#read in uncertainty table for additional sectors\n",
    "parameter_table_additional_sectors = pd.read_csv(sr.fp_csv_parameter_ranges)\n",
    "#reduce\n",
    "parameter_table_additional_sectors = parameter_table_additional_sectors[parameter_table_additional_sectors[\"type\"].isin([\"incertidumbre\", \"accion\"])]\n",
    "#add field\n",
    "parameter_table_additional_sectors[\"variable_name_lower\"] = [x.lower().replace(\" \", \"_\") for x in list(parameter_table_additional_sectors[\"parameter\"])]\n",
    "#fill nas and set to integer\n",
    "parameter_table_additional_sectors[\"parameter_constant_q\"] = parameter_table_additional_sectors[\"parameter_constant_q\"].fillna(0)\n",
    "parameter_table_additional_sectors[\"parameter_constant_q\"] = np.array(parameter_table_additional_sectors[\"parameter_constant_q\"]).astype(int)\n",
    "#initialize available groups\n",
    "groups_norm = set([int(x) for x in parameter_table_additional_sectors[\"normalize_group\"] if not np.isnan(x)])\n",
    "\n",
    "# IDENTIFY PARAMETERS THAT DO NOT VARY\n",
    "df_apn = parameter_table_additional_sectors[[\"variable_name_lower\", \"min_2050\", \"max_2050\"]].copy().drop_duplicates().reset_index(drop = True)\n",
    "#initialize the set\n",
    "all_params_novary = set({})\n",
    "#loop to build\n",
    "for p in df_apn[\"variable_name_lower\"].unique():\n",
    "    df_apn_tmp = df_apn[df_apn[\"variable_name_lower\"] == p]\n",
    "    #check to see if it doesn't vary\n",
    "    if len(df_apn_tmp) == 1:\n",
    "        if float(df_apn_tmp[\"min_2050\"].iloc[0]) == float(df_apn_tmp[\"max_2050\"].iloc[0]):\n",
    "            if float(df_apn_tmp[\"min_2050\"].iloc[0]) == 1.0:\n",
    "                all_params_novary = all_params_novary | set({p})\n",
    "\n",
    "\n",
    "\n",
    "###   NORMALIZATION GROUP IDS\n",
    "\n",
    "group_id = parameter_table_additional_sectors[[\"type\", \"parameter\", \"normalize_group\"]].drop_duplicates()\n",
    "#build normalize group and lever group ids (for deltas)\n",
    "norm_vec = []\n",
    "\n",
    "if len(groups_norm) > 0:\n",
    "    #starting point for new group\n",
    "    ind_group = max(groups_norm) + 1\n",
    "else:\n",
    "    ind_group = 1\n",
    "\n",
    "#loop over rows\n",
    "for i in range(0, len(group_id)):\n",
    "    #get current group\n",
    "    group_cur = group_id[\"normalize_group\"].iloc[i]\n",
    "    #test for NaN\n",
    "    if np.isnan(group_cur):\n",
    "        norm_vec = norm_vec + [ind_group]\n",
    "        #next iteration\n",
    "        ind_group += 1\n",
    "    else:\n",
    "        norm_vec = norm_vec + [int(group_cur)]\n",
    "\n",
    "#add to group_id data frame\n",
    "group_id[\"norm_group_id\"] = norm_vec\n",
    "group_id = group_id.reset_index(drop = True)\n",
    "\n",
    "\n",
    "###   LEVER GROUP IDS\n",
    "\n",
    "#temporary data frame to build lever group id with\n",
    "group_id_tmp = group_id[[\"type\", \"norm_group_id\"]].drop_duplicates()\n",
    "group_id_tmp = group_id_tmp[group_id_tmp[\"type\"].isin([\"Accion\", \"accion\"])]\n",
    "group_id_tmp[\"lever_group_id\"] = range(1, len(group_id_tmp) + 1)\n",
    "if \"lever_group_id\" not in group_id.columns:\n",
    "    group_id = pd.merge(group_id, group_id_tmp[[\"norm_group_id\", \"lever_group_id\"]], how = \"left\", left_on = [\"norm_group_id\"], right_on = [\"norm_group_id\"])\n",
    "#reduce\n",
    "group_id = group_id[[\"parameter\", \"type\", \"norm_group_id\", \"lever_group_id\"]]\n",
    "#replace nas\n",
    "group_id = group_id.fillna({\"lever_group_id\": -1})\n",
    "#integer\n",
    "group_id[\"lever_group_id\"] = [int(x) for x in list(group_id[\"lever_group_id\"])]\n",
    "#merge in\n",
    "group_id  = pd.merge(group_id, parameter_table_additional_sectors[[\"parameter\", \"variable_name_lower\"]], how = \"left\", left_on = [\"parameter\"], right_on = [\"parameter\"])\n",
    "group_id = group_id.drop_duplicates()\n",
    "group_id = group_id[[\"parameter\", \"type\", \"variable_name_lower\", \"norm_group_id\", \"lever_group_id\"]]\n",
    "\n",
    "\n",
    "###   MERGE BACK IN TO PTAS AND CLEAN NAMES/DATA\n",
    "\n",
    "#set merge fields and merge\n",
    "fields_merge = list(set(parameter_table_additional_sectors.columns).intersection(set(group_id.columns)))\n",
    "parameter_table_additional_sectors = pd.merge(parameter_table_additional_sectors, group_id, how = \"left\", left_on = fields_merge, right_on = fields_merge)\n",
    "#fields to extract\n",
    "fields_ext = [x for x in parameter_table_additional_sectors if (x not in [\"normalize_group\"])]\n",
    "#clean the data frame\n",
    "parameter_table_additional_sectors = parameter_table_additional_sectors[fields_ext]\n",
    "parameter_table_additional_sectors = parameter_table_additional_sectors.dropna(axis = 1)\n",
    "#dictionary to rename\n",
    "dict_ptas_rename = dict([[x, x.lower().replace(\" \", \"_\")] for x in parameter_table_additional_sectors.columns])\n",
    "#set type\n",
    "parameter_table_additional_sectors = parameter_table_additional_sectors.rename(columns = dict_ptas_rename)\n",
    "\n",
    "#parameters to index\n",
    "fields_add_sec_all_vals = [\"strategy_id\", \"type\", \"parameter\", \"sector\", \"norm_group_id\", \"lever_group_id\"]\n",
    "#get parameter years that are defined\n",
    "param_years_add_sec = [float(x) for x in parameter_table_additional_sectors.columns if x.replace(\".\", \"\").isnumeric()]\n",
    "param_years_add_sec = [int(x) for x in param_years_add_sec if (int(x) == x)]\n",
    "print(\"\\n\\nparam_years_add_sec:\")\n",
    "print(param_years_add_sec)\n",
    "print(parameter_table_additional_sectors.columns)\n",
    "print(\"\\n\\n\")\n",
    "#initialize for all parameters\n",
    "all_vals_add_sec = {\n",
    "    \"param_years\": param_years_add_sec,\n",
    "    \"future_id\": list(range(1, n_lhs + 1)),\n",
    "    \"design_id\": list(df_attribute_design_id[\"design_id\"]),\n",
    "    \"time_series_id\": list(set(df_attribute_time_series_id[\"time_series_id\"]) & set(parameter_table_additional_sectors[\"time_series_id\"]))\n",
    "}\n",
    "#sort some\n",
    "all_vals_add_sec[\"time_series_id\"].sort()\n",
    "\n",
    "#loop\n",
    "for field in fields_add_sec_all_vals:\n",
    "    #set the field name\n",
    "    str_field = field.lower().replace(\" \", \"_\")\n",
    "    #\n",
    "    if field in [\"lever_group_id\", \"norm_group_id\"]:\n",
    "        set_field = set([x for x in parameter_table_additional_sectors[field] if x > 0])\n",
    "    else:\n",
    "        set_field = set(parameter_table_additional_sectors[field])\n",
    "    #update the dictionary\n",
    "    all_vals_add_sec.update({str_field: set_field})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################\n",
    "#    GENERATE ATTRIBUTE TABLES    #\n",
    "###################################\n",
    "\n",
    "###   RUN ID\n",
    "df_attribute_run_id_0 = [(x, y) for x in sr.df_strat[\"strategy_id\"] for y in ([0] + all_vals_add_sec[\"future_id\"])]\n",
    "df_attribute_run_id = pd.DataFrame(df_attribute_run_id_0)\n",
    "df_attribute_run_id = df_attribute_run_id.rename(columns = {0: \"strategy_id\", 1: \"future_id\"})\n",
    "df_attribute_run_id[\"run_id\"] = range(0, len(df_attribute_run_id))\n",
    "df_attribute_run_id = df_attribute_run_id[[\"run_id\", \"strategy_id\", \"future_id\"]]\n",
    "\n",
    "if export_ed_files_q:\n",
    "    #note/export\n",
    "    print(\"Exporting run_id attribute to \" + sr.fp_csv_attribute_runs)\n",
    "    df_attribute_run_id.to_csv(sr.fp_csv_attribute_runs, index = None)\n",
    "\n",
    "###   MASTER ID\n",
    "df_attribute_master_id = [[x] + [y] + list(z) for x in all_vals_add_sec[\"design_id\"] for y in all_vals_add_sec[\"time_series_id\"] for z in df_attribute_run_id_0]\n",
    "df_attribute_master_id = pd.DataFrame(df_attribute_master_id)\n",
    "df_attribute_master_id = df_attribute_master_id.rename(columns = {0: \"design_id\", 1: \"time_series_id\", 2: \"strategy_id\", 3: \"future_id\"})\n",
    "fields_df_attribute_master_id = list(df_attribute_master_id.columns)\n",
    "df_attribute_master_id[\"master_id\"] = list(range(0, len(df_attribute_master_id)))\n",
    "df_attribute_master_id = df_attribute_master_id[[\"master_id\"] + fields_df_attribute_master_id]\n",
    "df_attribute_master_id = pd.merge(df_attribute_master_id, df_attribute_run_id, how = \"left\", on = [\"strategy_id\", \"future_id\"])\n",
    "#fields to order by\n",
    "fields_ord_dfm = [\"master_id\", \"design_id\", \"time_series_id\", \"run_id\", \"strategy_id\", \"future_id\"]\n",
    "#reorder\n",
    "df_attribute_master_id = df_attribute_master_id[fields_ord_dfm].sort_values(by = fields_ord_dfm)\n",
    "\n",
    "if export_ed_files_q:\n",
    "    #note/export\n",
    "    print(\"Exporting master_id attribute to \" + sr.fp_csv_attribute_master)\n",
    "    #export\n",
    "    df_attribute_master_id.to_csv(sr.fp_csv_attribute_master, index = None, encoding = \"UTF-8\")\n",
    "    #export for gams\n",
    "    df_attribute_master_id_gams = df_attribute_master_id[[\"master_id\"]].copy().rename(columns = {\"master_id\": \"Escenarios\"})\n",
    "    df_attribute_master_id_gams.to_csv(sr.fp_csv_gams_data_set_escenarios, index = None, encoding = \"UTF-8\")\n",
    "    del df_attribute_master_id_gams\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "#    GENERATE LHS MATRIX    #\n",
    "#############################\n",
    "\n",
    "print(\"Generating LHS Matrix...\")\n",
    "print(\"\")\n",
    "#dimensions\n",
    "p_add_sec = len(all_vals_add_sec[\"parameter\"])\n",
    "n_add_sec_levers = len(all_vals_add_sec[\"lever_group_id\"])\n",
    "#generate latin hypercube sample\n",
    "matrix_lhs = pyd.lhs(p_add_sec + n_add_sec_levers, samples = n_lhs)\n",
    "#vector for delineating sub-matrices\n",
    "vec_submat_lhs = [0, p_add_sec, n_add_sec_levers]\n",
    "#set vector of submatrix index\n",
    "vec_submat_lhs_names = [\"add_sec\", \"levers\"]\n",
    "\n",
    "##  set names for additional sectors\n",
    "dict_names_add_sec = [x.lower().replace(\" \", \"_\") for x in all_vals_add_sec[\"parameter\"]]\n",
    "dict_names_add_sec.sort()\n",
    "dict_names_add_sec = [[x, dict_names_add_sec[x]] for x in range(0, len(dict_names_add_sec))]\n",
    "dict_names_add_sec = dict(dict_names_add_sec)\n",
    "\n",
    "##  set names for lever groups\n",
    "dict_names_levers = [\"lever_group_\" + str(x) for x in list(all_vals_add_sec[\"lever_group_id\"])]\n",
    "dict_names_levers = [[x, dict_names_levers[x]] for x in range(0, len(dict_names_levers))]\n",
    "dict_names_levers = dict(dict_names_levers)\n",
    "\n",
    "#initialize the fields for data frames\n",
    "dict_submat_lhs_fields = {\n",
    "    \"add_sec\": dict_names_add_sec,\n",
    "    \"levers\": dict_names_levers\n",
    "}\n",
    "#setup the output file paths\n",
    "dict_submat_file_paths = {\n",
    "    \"add_sec\": sr.fp_csv_lhs_table_multi_sector,\n",
    "    \"levers\": sr.fp_csv_lhs_table_levers\n",
    "}\n",
    "\n",
    "#initialize\n",
    "dict_submat_lhs = {}\n",
    "\n",
    "if not sr.read_lhs_tables_q:\n",
    "    print(\"\\n#####\\n#####    GENERATING LHS TABLES WITH \" + str(n_lhs) + \" SAMPLES\\n#####\\n\")\n",
    "\n",
    "    #break off components\n",
    "    for i in range(0, len(vec_submat_lhs) - 1):\n",
    "        #set the field name for the dictionary\n",
    "        nm = vec_submat_lhs_names[i]\n",
    "        #check\n",
    "        if i > -1:\n",
    "            #get the indeces\n",
    "            p0 = sum(vec_submat_lhs[0:(i + 1)])\n",
    "        else:\n",
    "            p0 = 0\n",
    "        #set upper limit\n",
    "        p1 = sum(vec_submat_lhs[0:(i + 2)])\n",
    "        #temporary dataframe\n",
    "        df_tmp = pd.DataFrame(matrix_lhs[:, p0:p1], index = None)\n",
    "        #add name field\n",
    "        if dict_submat_lhs_fields[nm] != None:\n",
    "            df_tmp = df_tmp.rename(columns = dict_submat_lhs_fields[nm])\n",
    "            #set names\n",
    "            nms = list(df_tmp.columns)\n",
    "            #add run id\n",
    "            df_tmp[\"future_id\"] = all_vals_add_sec[\"future_id\"]\n",
    "            #reorder\n",
    "            df_tmp = df_tmp[[\"future_id\"] + nms]\n",
    "        #update dictionary\n",
    "        dict_submat_lhs.update({nm: df_tmp})\n",
    "\n",
    "        #export raw lhs data\n",
    "        if export_ed_files_q:\n",
    "            #note/export\n",
    "            print(\"Exporting LHS for \" + nm + \" to \" + dict_submat_file_paths[nm])\n",
    "            df_tmp.to_csv(dict_submat_file_paths[nm], index = None)\n",
    "\n",
    "else:\n",
    "    #notify\n",
    "    print(\"\\n#####\\n#####    READING IN LHS TABLES\\n#####\\n\")\n",
    "    #initialize list of \"future ids\"\n",
    "    set_future_ids_read = set({})\n",
    "    ##  initialie booleans\n",
    "\n",
    "    #initialize the set of futures that are read\n",
    "    init_sfir_q = True\n",
    "    #default the imbalance query to fale\n",
    "    set_imbalance_q = False\n",
    "    #default exitting to false\n",
    "    exit_q = False\n",
    "\n",
    "    #Initialize\n",
    "    dict_read_futures = {}\n",
    "    #exit codes\n",
    "    dict_exit_codes = {\n",
    "        \"set_imbalance\": \"Number of future_ids in LHS tables are not the same. Check the LHS files to ensure they are using the same future_id indexing.\",\n",
    "        \"set_nomatch\": \"LHS Tables have future ids that do not match specificed number of lhs trials.\"\n",
    "    }\n",
    "    #initialize index\n",
    "    i = 0\n",
    "    #read in lhs tables\n",
    "    while (i < len(vec_submat_lhs_names)) and not set_imbalance_q:\n",
    "        #get current file\n",
    "        nm = str(vec_submat_lhs_names[i])\n",
    "        #get file path\n",
    "        fp_read = dict_submat_file_paths[nm]\n",
    "        #read it in\n",
    "        df_tmp = pd.read_csv(fp_read)\n",
    "        #reorder it\n",
    "        #df_tmp = df_tmp[dict_submat_lhs_fields[nm]]\n",
    "        #update dictionary\n",
    "        dict_submat_lhs.update({nm: df_tmp})\n",
    "        #check\n",
    "        if init_sfir_q:\n",
    "            #initialize\n",
    "            set_future_ids_read = set(df_tmp[\"future_id\"])\n",
    "            #set of futures to compare to for individual exit\n",
    "            set_future_ids_read_compare = set_future_ids_read\n",
    "            #turn off initialization\n",
    "            init_sfir_q = False\n",
    "        else:\n",
    "            #current set of Future IDs\n",
    "            set_future_ids_read_cur = set(df_tmp[\"future_id\"])\n",
    "            #read in and update the set of intersectional futures\n",
    "            set_future_ids_read = set_future_ids_read & set_future_ids_read_cur\n",
    "            #check\n",
    "            if set_future_ids_read_cur != set_future_ids_read_compare:\n",
    "                #if any set of futures doesn't match the first one, turn on the exit\n",
    "                sys.exit(dict_exit_codes[\"set_imbalance\"])\n",
    "        #notify of successful completion\n",
    "        print(nm + \" LHS table successfully read from \" + fp_read)\n",
    "        #next ieration\n",
    "        i += 1\n",
    "\n",
    "    #cut out 0 (some files may have it, some may not)\n",
    "    set_future_ids_read = set_future_ids_read - set({0})\n",
    "    #set of what should be the future ids\n",
    "    set_check_future_ids = set(range(1, n_lhs + 1))\n",
    "    #compare\n",
    "    if set_future_ids_read != set_check_future_ids:\n",
    "        sys.exit(dict_exit_codes[\"set_nomatch\"])\n",
    "\n",
    "print(\"\\nLHS complete.\\n\")\n",
    "\n",
    "\n",
    "##  SET SOME NAMES\n",
    "fields_ed_add_sec = dict_submat_lhs[\"add_sec\"].columns\n",
    "fields_ordered_parameters = [x for x in fields_ed_add_sec if x != \"future_id\"]\n",
    "sr.print_list_output(fields_ordered_parameters, \"fields_ordered_parameters\")\n",
    "\n",
    "#get parameters that are not rampedâ€”they are constant across all years\n",
    "all_constant_params = set(parameter_table_additional_sectors[parameter_table_additional_sectors[\"parameter_constant_q\"] == 1][\"parameter\"])\n",
    "all_constant_params = list(all_constant_params)\n",
    "all_constant_params.sort()\n",
    "\n",
    "sr.print_list_output(all_constant_params, \"all_constant_params\")\n",
    "\n",
    "#get indices\n",
    "indices_fop_all_constant_params = [fields_ordered_parameters.index(x) for x in all_constant_params]\n",
    "\n",
    "\n",
    "#################################################################\n",
    "###                                                           ###\n",
    "###    GENERATE EXPERIMENTAL DESIGN FOR ADDITIONAL SECTORS    ###\n",
    "###                                                           ###\n",
    "#################################################################\n",
    "\n",
    "###############################\n",
    "#    GENERATE LEVER DELTAS    #\n",
    "###############################\n",
    "\n",
    "print(\"Starting generation of lever deltas.\")\n",
    "print(\"\")\n",
    "##  START BUY BUILDING LONG TABLE OF TRANSFORMED LHS SAMPLES\n",
    "\n",
    "dsl = dict_submat_lhs[\"levers\"].copy()\n",
    "dsl = pd.wide_to_long(dsl, i = [\"future_id\"], j = \"lever_group_id\", stubnames = \"lever_group_\")\n",
    "dsl = dsl.reset_index()\n",
    "dsl = dsl.rename(columns = {\"lever_group_\": \"lhs_val\"})\n",
    "#data frame out\n",
    "df_ld_lhs_transformed = []\n",
    "#loop over design id\n",
    "for did in all_vals_add_sec[\"design_id\"]:\n",
    "    #get applicable data\n",
    "    dict_data = df_attribute_design_id[df_attribute_design_id[\"design_id\"] == did].to_dict()\n",
    "    #initialize\n",
    "    lhs_out = dsl.copy()\n",
    "    #get key for to_dict\n",
    "    key = list(dict_data[\"vary_lever_deltas\"].keys())[0]\n",
    "    #get range of values\n",
    "    vec_vals = np.array(lhs_out[\"lhs_val\"])\n",
    "    #set header for Future 0\n",
    "    df_fut_0 = pd.DataFrame([[did, 0, x, 1] for x in all_vals_add_sec[\"lever_group_id\"]], columns = [\"design_id\", \"future_id\", \"lever_group_id\", \"lhs_val\"])\n",
    "    #check on varying\n",
    "    if dict_data[\"vary_lever_deltas\"][key] == 1:\n",
    "        #transform\n",
    "        m = float(dict_data[\"linear_transform_ld_m\"][key])\n",
    "        b = float(dict_data[\"linear_transform_ld_b\"][key])\n",
    "        #thresholds\n",
    "        thresh_min = float(dict_data[\"min_lever_deltas\"][key])\n",
    "        thresh_max = float(dict_data[\"max_lever_deltas\"][key])\n",
    "        #transformation\n",
    "        def linear_transform(x):\n",
    "            return max(min(m*x + b, thresh_max), thresh_min)\n",
    "        #updated vals\n",
    "        vec_vals = list(map(linear_transform, vec_vals))\n",
    "    else:\n",
    "        #set the deltas to 1\n",
    "        vec_vals = [1.0 for x in range(len(vec_vals))]\n",
    "    #update in data frames\n",
    "    lhs_out[\"lhs_val\"] = vec_vals\n",
    "    lhs_out[\"design_id\"] = [did for x in range(len(vec_vals))]\n",
    "    #update\n",
    "    lhs_out = pd.concat([df_fut_0, lhs_out[[\"design_id\", \"future_id\", \"lever_group_id\", \"lhs_val\"]]])\n",
    "    #add\n",
    "    df_ld_lhs_transformed = df_ld_lhs_transformed + [lhs_out]\n",
    "#convert to dataframee\n",
    "df_ld_lhs_transformed = pd.concat(df_ld_lhs_transformed)\n",
    "\n",
    "\n",
    "##  THEN, GO BY STRATEGY TO BUILD DELTAS\n",
    "\n",
    "print(\"Starting build of deltas by time series/strategy...\")\n",
    "print(\"\")\n",
    "#set strategies to build deltas for\n",
    "strat_lever_deltas = [x for x in all_vals_add_sec[\"strategy_id\"] if x != strat_baseline]\n",
    "#temporary reduction\n",
    "ptas_ld = parameter_table_additional_sectors[parameter_table_additional_sectors[\"lever_group_id\"] > 0]\n",
    "\n",
    "#merge field for generate lever deltas\n",
    "fields_merge_ld = [\"variable_name_lower\", \"lever_group_id\"]\n",
    "#set extraction fields by type\n",
    "extraction_fields_ld = fields_merge_ld + [str(x) for x in param_years_add_sec]\n",
    "\n",
    "#dictionary of lever deltas by strategy\n",
    "dict_ld = {}\n",
    "#fields to extract and use in transpose\n",
    "fields_ext_ld = fields_ordered_parameters\n",
    "#initialize\n",
    "df_ld_shaped = []\n",
    "#loop over time series ids\n",
    "for ts_id in all_vals_add_sec[\"time_series_id\"]:\n",
    "    #get baseline data frame\n",
    "    df_base = ptas_ld[(ptas_ld[\"strategy_id\"] == strat_baseline) & (ptas_ld[\"time_series_id\"] == ts_id)].copy()\n",
    "    #reduce\n",
    "    df_base = df_base[extraction_fields_ld]\n",
    "\n",
    "    #loop over strategies to generate lever deltas by included years\n",
    "    for strat in strat_lever_deltas:\n",
    "        #get strategy id\n",
    "        strat_id = int(strat)#int(sr.dict_strat_ids[strat])\n",
    "        #get sub data frame\n",
    "        df_strat = ptas_ld[(ptas_ld[\"strategy_id\"] == strat) & (ptas_ld[\"time_series_id\"] == ts_id)].copy()\n",
    "        df_strat = df_strat[extraction_fields_ld]\n",
    "        #data fields\n",
    "        fields_data = [x for x in extraction_fields_ld if (x not in fields_merge_ld)]\n",
    "        #get column rename\n",
    "        dict_rename_df_strat = dict([[str(x), str(strat) + \"_\" + str(x)] for x in fields_data])\n",
    "        #update\n",
    "        df_strat = df_strat.rename(columns = dict_rename_df_strat)\n",
    "        #merge in\n",
    "        df_strat = pd.merge(df_strat, df_base, how = \"inner\", left_on = fields_merge_ld, right_on = fields_merge_ld)\n",
    "        #set new lever delta fields\n",
    "        fields_ld = []\n",
    "        #generate subtraction\n",
    "        for fd in fields_data:\n",
    "            field_0 = fd\n",
    "            field_s = str(strat) + \"_\" + str(fd)\n",
    "            field_ld = \"ld_\" + field_s\n",
    "            fields_ld = fields_ld + [field_ld]\n",
    "            #parse out and get difference\n",
    "            df_strat_tmp = df_strat[[field_0, field_s]].diff(axis = 1)\n",
    "            #update\n",
    "            df_strat[field_ld] = df_strat_tmp[field_s]\n",
    "        #reduce\n",
    "        df_strat = df_strat[fields_merge_ld + fields_ld]\n",
    "        #merge in\n",
    "        df_strat = pd.merge(df_strat, df_ld_lhs_transformed, how = \"outer\", left_on = [\"lever_group_id\"], right_on = [\"lever_group_id\"])\n",
    "        #build new data\n",
    "        df_deltas_adj = (np.array(df_strat[fields_ld]).transpose() * np.array(df_strat[\"lhs_val\"])).transpose()\n",
    "        df_deltas_adj = pd.DataFrame(df_deltas_adj, columns = fields_ld)\n",
    "        #re-initialize\n",
    "        df_strat = df_strat[[\"design_id\", \"future_id\"] + fields_merge_ld]\n",
    "        df_strat = pd.concat([df_strat, df_deltas_adj], axis = 1, sort = False)\n",
    "        #update\n",
    "        dict_ld.update({strat: df_strat})\n",
    "\n",
    "        # BUILD RESHAPED VALUE\n",
    "\n",
    "        #loop over design/future\n",
    "        for did in list(df_strat[\"design_id\"].unique()):\n",
    "            for fut in list(df_strat[\"future_id\"].unique()):\n",
    "                df_tmp = df_strat[(df_strat[\"design_id\"] == did) & (df_strat[\"future_id\"] == fut)].copy()\n",
    "                df_tmp = df_tmp[[\"variable_name_lower\"] + fields_ld]\n",
    "                #fields to add\n",
    "                fields_new = list(df_tmp[\"variable_name_lower\"])\n",
    "                #new data frame\n",
    "                df_tmp = pd.DataFrame(np.array(df_tmp[fields_ld]).transpose(), columns = fields_new)\n",
    "                #add year\n",
    "                df_tmp[\"year\"] = param_years_add_sec\n",
    "                #add design id and future\n",
    "                df_tmp[\"design_id\"] = [did for x in range(len(df_tmp))]\n",
    "                df_tmp[\"future_id\"] = [fut for x in range(len(df_tmp))]\n",
    "                df_tmp[\"strategy_id\"] = [strat_id for x in range(len(df_tmp))]\n",
    "                df_tmp[\"time_series_id\"] = [ts_id for x in range(len(df_tmp))]\n",
    "                #order\n",
    "                df_tmp = df_tmp[[\"design_id\", \"time_series_id\", \"strategy_id\", \"future_id\", \"year\"] + fields_new]\n",
    "                #update\n",
    "                df_ld_shaped = df_ld_shaped + [df_tmp]\n",
    "            #notify of completed reshape\n",
    "            print(\"Reshaping of LHS table complete for design_id: \" + str(did) + \", time_series_id: \" + str(ts_id) + \", strategy_id: \" + str(strat))\n",
    "\n",
    "if len(df_ld_shaped) > 0:\n",
    "    #convert to wide frame\n",
    "    df_ld_shaped = pd.concat(df_ld_shaped)\n",
    "    #rename\n",
    "    df_ld_shaped = df_ld_shaped.rename(columns = dict([[x, \"ld_\" + x] for x in fields_new]))\n",
    "\n",
    "\n",
    "\n",
    "##  CREATE RAMP VECTORS FOR UNCERTAINTY\n",
    "\n",
    "y_0 = int(sr.dict_init[\"add_sec_variation_start_year\"]) - 1\n",
    "y_1 = max(param_years_add_sec)\n",
    "y_base = min(param_years_add_sec)\n",
    "vec_ramp_unc = sr.build_linear_mix_vec((y_0, y_1), (y_base, y_1))\n",
    "#sr.build_mix_vec(0, 0, 0, \"linear\")#np.array([max(min((y - y_0)/(y_1 - y_0), 1), 0) for y in param_years_add_sec])\n",
    "vec_ramp_base = 1 - vec_ramp_unc\n",
    "\n",
    "\n",
    "##  BUILD BASIC PERCENTAGE CHANGE MATRIX\n",
    "\n",
    "\n",
    "#for ts_id in all_vals_add_sec[\"time_series_id\"]:\n",
    "df_ed_baseline = parameter_table_additional_sectors[(parameter_table_additional_sectors[\"strategy_id\"] == strat_baseline) & (parameter_table_additional_sectors[\"time_series_id\"] == 0)].copy()\n",
    "df_ed_base = df_ed_baseline[extraction_fields_ld].copy()\n",
    "df_ed_add_sec = dict_submat_lhs[\"add_sec\"]\n",
    "#initialize array (in order of ext fields)\n",
    "array_ed_add_sec = np.array(df_ed_add_sec[fields_ordered_parameters])\n",
    "#order output array\n",
    "df_max_min = pd.merge(pd.DataFrame(fields_ordered_parameters, columns = [\"variable_name_lower\"]), df_ed_baseline[[\"variable_name_lower\", \"min_2050\", \"max_2050\"]], how = \"left\", left_on = [\"variable_name_lower\"], right_on = [\"variable_name_lower\"])\n",
    "#generate transformed values of uncertainty\n",
    "array_ed_add_sec_trans = array_ed_add_sec * np.array(df_max_min[\"max_2050\"]) + (1 - array_ed_add_sec) * np.array(df_max_min[\"min_2050\"])\n",
    "#add in future 0 (0 change = 100%)\n",
    "array_ed_add_sec_trans = np.concatenate([np.ones((1, array_ed_add_sec_trans.shape[1])), array_ed_add_sec_trans])\n",
    "#set data frame of baseline percentage changes\n",
    "df_perc_change_unc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build outcome matrix\n",
    "for i in range(len(param_years_add_sec)):\n",
    "    y = param_years_add_sec[i]\n",
    "    frac_base = vec_ramp_base[i]\n",
    "    frac_unc = vec_ramp_unc[i]\n",
    "    #build weighted array\n",
    "    array_tmp = np.ones(array_ed_add_sec_trans.shape)*(frac_base) + array_ed_add_sec_trans*(frac_unc)\n",
    "    #NOTE: FOR CONSTANT PARAMS, GET RID OF \"RAMP\"\n",
    "    array_tmp[:, indices_fop_all_constant_params] = array_ed_add_sec_trans[:, indices_fop_all_constant_params].copy()\n",
    "    #convert to data frame\n",
    "    df_tmp = pd.DataFrame(array_tmp, columns = fields_ordered_parameters)\n",
    "    #add year\n",
    "    df_tmp[\"year\"] = [int(y) for x in range(len(df_tmp))]\n",
    "    df_tmp[\"future_id\"] = [0] + list(df_ed_add_sec[\"future_id\"])\n",
    "    #df_tmp[\"time_series_id\"] = [ts_id for x in range(len(df_tmp))]\n",
    "    #organize\n",
    "    df_tmp = df_tmp[[\"future_id\", \"year\"] + fields_ordered_parameters]\n",
    "    #update\n",
    "    df_perc_change_unc = df_perc_change_unc + [df_tmp]\n",
    "#build master\n",
    "df_perc_change_unc = pd.concat(df_perc_change_unc)\n",
    "df_perc_change_unc = df_perc_change_unc.sort_values(by = [\"future_id\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.76149981, 0.88567848, 0.91123552, 0.83531655,\n",
       "       0.81896796, 0.79317553, 0.7752458 , 0.73407119, 0.97847245,\n",
       "       1.09644345, 0.84722847, 0.80919952, 1.08186397, 0.94511067,\n",
       "       0.70619781, 0.83677039, 0.91232671, 0.98219821, 1.08961278,\n",
       "       0.95654469, 0.75999742, 0.93029581, 1.00594151, 0.72166721,\n",
       "       0.86052144, 0.72665706, 0.78320479, 1.05059958, 0.82795556,\n",
       "       1.03759511, 1.02262575, 0.85724724, 1.05971627, 0.85112961,\n",
       "       1.04349469, 0.93386197, 0.93613298, 0.85346281, 0.99567198,\n",
       "       1.07799856, 0.8836382 , 0.98896572, 1.01356819, 0.94367058,\n",
       "       0.75544851, 0.90447088, 0.86500704, 0.76433571, 0.71825394,\n",
       "       0.87168095, 0.92251938, 0.70189391, 0.99795205, 0.8148212 ,\n",
       "       0.95090717, 0.89381073, 0.89771269, 1.07242863, 0.74142407,\n",
       "       1.01768434, 1.06358258, 0.74731685, 1.0468096 , 1.06867034,\n",
       "       1.01090727, 1.09449355, 0.80370193, 0.97472842, 0.74958509,\n",
       "       0.84385814, 0.78672528, 0.90253419, 0.7994858 , 0.96424603,\n",
       "       1.00077955, 0.71360791, 1.03421859, 1.05536501, 0.73679406,\n",
       "       0.7101614 , 0.95258991, 0.9256989 , 0.89041138, 0.82369941,\n",
       "       0.7791676 , 0.79185681, 1.06759221, 0.87509764, 0.9610168 ,\n",
       "       0.83047756, 1.02460415, 0.87927158, 0.76845239, 0.80762447,\n",
       "       1.03002581, 0.91743018, 0.98499099, 0.96873522, 1.08634411,\n",
       "       0.7316629 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_perc_change_unc[\"medida_biodigestores\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building data frame basis for experimental design file...\n",
      "\n",
      "Data frame for time_series_id: 1, design_id: 0 complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##  BUILD DATA FRAME BASIS FOR EXPERIMENTAL DESIGN FILE\n",
    "\n",
    "print(\"Building data frame basis for experimental design file...\")\n",
    "print(\"\")\n",
    "\n",
    "if len(df_ld_shaped) > 0:\n",
    "    #fields that are in the LD matrix\n",
    "    fields_ld_data = [x for x in df_ld_shaped.columns if (x.replace(\"ld_\", \"\") in fields_ordered_parameters)]\n",
    "\n",
    "    #print(\"\\n\"*8 + \"#\"*30)\n",
    "    #print(\"\\nSUCCESS\\n\\n\")\n",
    "    #print(df_ld_shaped)\n",
    "    #print(\"\\n\\n\" + \"#\"*30 + \"\\n\"*8)\n",
    "else:\n",
    "    fields_ld_data = []\n",
    "fields_ldparams_data = [x.replace(\"ld_\", \"\") for x in fields_ld_data]\n",
    "fields_nonldparams_data = [x for x in fields_ordered_parameters if (x not in fields_ldparams_data)]\n",
    "#initialize output data frame\n",
    "df_out = []\n",
    "#loop over time series\n",
    "for ts_id in [1]:#all_vals_add_sec[\"time_series_id\"]:\n",
    "    #get baseline for each time series\n",
    "    df_ed_baseline = parameter_table_additional_sectors[(parameter_table_additional_sectors[\"strategy_id\"] == strat_baseline) & (parameter_table_additional_sectors[\"time_series_id\"] == ts_id)].copy()\n",
    "    df_ed_base = df_ed_baseline[extraction_fields_ld].copy()\n",
    "    #set column headers\n",
    "    fields_df_ed = list(df_ed_base[\"variable_name_lower\"])\n",
    "    df_ed_base = df_ed_base.transpose().loc[[str(x) for x in param_years_add_sec],:]\n",
    "    df_ed_base = df_ed_base.rename(columns = dict([[list(df_ed_base.columns)[x], fields_df_ed[x]] for x in range(len(fields_df_ed))]))\n",
    "    df_ed_base[\"year\"] = [int(x) for x in df_ed_base.index]\n",
    "\n",
    "    #loop over design ids\n",
    "    for did in [0]:#all_vals_add_sec[\"design_id\"]:\n",
    "        #get applicable data\n",
    "        dict_data = df_attribute_design_id[df_attribute_design_id[\"design_id\"] == did].to_dict()\n",
    "        #get key for to_dict\n",
    "        key = list(dict_data[\"vary_lever_deltas\"].keys())[0]\n",
    "        #experimental design merge table\n",
    "        df_ed_merge = df_attribute_master_id[(df_attribute_master_id[\"time_series_id\"] == ts_id) & (df_attribute_master_id[\"design_id\"] == did)]\n",
    "        list_ed_merge = []\n",
    "        cols = [x for x in df_ed_merge.columns] + [\"year\"]\n",
    "        for y in param_years_add_sec:\n",
    "            df_tmp = df_ed_merge.copy()\n",
    "            df_tmp[\"year\"] = [int(y) for x in range(len(df_tmp))]\n",
    "            df_tmp = df_tmp[cols]\n",
    "            list_ed_merge = list_ed_merge + [df_tmp]\n",
    "        #convert\n",
    "        df_ed_merge = pd.concat(list_ed_merge)\n",
    "        #add in\n",
    "        df_ed_merge = pd.merge(df_ed_merge, df_ed_base, how = \"outer\", left_on = [\"year\"], right_on = [\"year\"])\n",
    "        #renaming dictionary\n",
    "        dict_rnm = dict([x, \"perc_\" + x] for x in fields_ordered_parameters)\n",
    "        #breakout fields\n",
    "        fields_percs = [dict_rnm[x] for x in fields_ordered_parameters]\n",
    "\n",
    "        #check if percentages need to be accounted for\n",
    "        if dict_data[\"vary_uncertainties\"][key] == 1:\n",
    "            df_tmp = df_perc_change_unc\n",
    "            #rename to columns\n",
    "            df_tmp = df_tmp.rename(columns = dict([x, \"perc_\" + x] for x in fields_ordered_parameters))\n",
    "            df_ed_merge = pd.merge(df_ed_merge, df_tmp, how = \"left\", left_on = [\"future_id\", \"year\"], right_on = [\"future_id\", \"year\"])\n",
    "            #id fields to breakout on\n",
    "            fields_id = [x for x in df_ed_merge.columns if (x not in fields_percs) and (x not in fields_ordered_parameters)]\n",
    "            #seprate out\n",
    "            df_ed_merge_ids = df_ed_merge[fields_id]\n",
    "            #convert to product arrayâ€”has headers fields_ordered_parameters\n",
    "            array_design = np.array(df_ed_merge[fields_percs]) * np.array(df_ed_merge[fields_ordered_parameters])\n",
    "        else:\n",
    "            #id fields to breakout on\n",
    "            fields_id = [x for x in df_ed_merge.columns if (x not in fields_ordered_parameters)]\n",
    "            #seprate out\n",
    "            df_ed_merge_ids = df_ed_merge[fields_id]\n",
    "            #reduce the design\n",
    "            array_design = np.array(df_ed_merge[fields_ordered_parameters])\n",
    "        #update\n",
    "        df_ed_merge = pd.concat([df_ed_merge_ids, pd.DataFrame(array_design, columns = fields_ordered_parameters)], axis = 1, sort = False)\n",
    "\n",
    "        #check if lever delta needs to be brought in\n",
    "        #if dict_data[\"vary_lever_deltas\"][key] == 1:\n",
    "\n",
    "        # ADD IN LEVER DELTAS\n",
    "\n",
    "        if len(df_ld_shaped) > 0:\n",
    "            #fields to merge on\n",
    "            fields_merge_ld = list(set(df_ed_merge.columns) & set(df_ld_shaped.columns))\n",
    "        else:\n",
    "            fields_merge_ld = []\n",
    "\n",
    "        if len(fields_merge_ld) > 0:\n",
    "            #merge in lever deltas\n",
    "            df_ed_merge = pd.merge(df_ed_merge, df_ld_shaped, how = \"left\", left_on = fields_merge_ld, right_on = fields_merge_ld)\n",
    "        #split out\n",
    "        df_ed_merge_ids = df_ed_merge[fields_id + fields_nonldparams_data]\n",
    "        #add in lever deltas\n",
    "        array_design = np.array(df_ed_merge[fields_ldparams_data]) + np.array(np.array(df_ed_merge[fields_ld_data].fillna(0)))\n",
    "        #concatenate\n",
    "        df_ed_merge = pd.concat([df_ed_merge_ids, pd.DataFrame(array_design, columns = fields_ldparams_data)], axis = 1, sort = False)\n",
    "\n",
    "        #reduce\n",
    "        df_ed_merge = df_ed_merge[df_ed_merge[\"strategy_id\"].isin(all_vals_add_sec[\"strategy_id\"])]\n",
    "        #sort\n",
    "        df_ed_merge = df_ed_merge.sort_values(by = [\"master_id\", \"year\"])\n",
    "        #clear index\n",
    "        df_ed_merge = df_ed_merge.reset_index(drop = True)\n",
    "        #order\n",
    "        df_ed_merge = df_ed_merge[fields_id + fields_ordered_parameters]\n",
    "\n",
    "        if len(df_out) == 0:\n",
    "            #add to output list\n",
    "            df_out = df_out + [df_ed_merge]\n",
    "        else:\n",
    "            df_out = df_out + [df_ed_merge[df_out[0].columns]]\n",
    "\n",
    "        print(\"Data frame for time_series_id: \" + str(ts_id) + \", design_id: \" + str(did) + \" complete.\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              1\n",
       "1              1\n",
       "2              1\n",
       "3              1\n",
       "4              1\n",
       "          ...   \n",
       "7267    0.731663\n",
       "7268    0.731663\n",
       "7269    0.731663\n",
       "7270    0.731663\n",
       "7271    0.731663\n",
       "Name: medida_biodigestores, Length: 7272, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ed_merge[\"medida_biodigestores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
